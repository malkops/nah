# Домашнее задание к занятию 17 «Инцидент-менеджмент»

## Основная часть

Составьте постмортем на основе реального сбоя системы GitHub в 2018 году.

Информация о сбое: 

* [в виде краткой выжимки на русском языке](https://habr.com/ru/post/427301/);
* [развёрнуто на английском языке](https://github.blog/2018-10-30-oct21-post-incident-analysis/).


|  Событие  |  Описание |
|---|---|
|  Краткое описание ицидента  |  Плановые работы по замене отказавшего оптического оборудования 100G привели к потере связи между нашим сетевым узлом на Восточном побережье США и основным центром обработки данных на Восточном побережье США, что в итоге вылилось к ухудшению обслуживания на 24 часа 11 минут  |
|  Причина инцидента  |  Из-за короткой сетевой недоступности Восточного побережья США, Orchestrator запустил процесс назначения лидера согласно Raft. Был назначен новый лидер и кворум, но так как Восточное и Западное побередье имели уникальные данные, не получилось перенести эти данные на первичный сервер |
|  Воздействие  |  Инцидент повлиял только на метаданные веб-сайтов, хранящиеся в базах данных MySQL, такие как Issue и PR. Были приостановлены работа веб-перехватчиков и сборка GitHub Pages  |
|  Обнаружение  |  Системы мониторинга начали генерировать оповещения о том, что в происходят многочисленные сбои  |
|  Реакция  |  Дежурная команда вручную заблокировала внутренний инструмент развертывания, чтобы предотвратить внесение каких-либо дополнительных изменений, также было частично ухудшено удобство использования сайта. Были привлечены: инженеры группы быстрого реагирования, координатор инцидента, инженеры из группы разработки  |
|  Восстановление  |   |
|  Таймлайн  |  21 октября в 22:52 UTC - Регламентные работы по замене вышедшего из строя оптического оборудования 100G<br>21 октября 22:52 UTC - Согласно консенсусу Raft сети Orchestrator начато аварийное переключение кластеров для направления операций записи в центр обработки данных на западном побережье США.<br>21 октября 22:54 UTC - Внутренние системы мониторинга начали генерировать предупреждения, указывающие на многочисленные сбои в наших системах.<br>21 октября 23:02 UTC - Инженеры группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в непредвиденном состоянии.<br>21 октября 23:07 UTC - Отвечающая команда вручную заблокировала внутренний инструмент развертывания<br>21 октября 23:09 UTC - Команда респондентов поместила сайт в желтый статус<br>21 октября 23:11 Присоединился координатор инцидента и через две минуты изменил статус на красный.<br>21 октября 23:13 UTC - Стало понятно, что проблема затронула несколько кластеров баз данных. Были вызваны дополнительные инженеры из группы разработки баз данных GitHub.<br>21 октября 23:19 UTC - Остановлено выполнение заданий, записывающих метаданные о таких вещах, как push-уведомления, приостановлено выполнение веб-перехватчика и сборка GitHub Pages.<br>22 октября 00:05 UTC - Инженеры, начали разработку плана по устранению несоответствий данных и внедрению процедур аварийного переключения для MySQL, обновилен статус, чтобы проинформировать пользователей о контролируемом переходе на другой узел кластера<br>22 октября 00:41 UTC - Инициирован процесс резервного копирования для всех затронутых кластеров MySQL.<br>22 октября 06:51 UTC - Несколько кластеров завершили восстановление из резервных копий в ЦОД на востоке США и начали репликацию новых данных с запада.<br>22 октября 07:46 UTC - GitHub опубликовал сообщение в блоге , чтобы предоставить больше информации.<br>22 октября 11:12 UTC - Все первичные базы данных снова установлены на восточном побережье США.<br>22 октября 13:15 UTC - Cтали доступны, дополнительные реплики чтения MySQL в общедоступном облаке восточного побережья США.<br>22 октября 16:24 UTC - Реплики синхронизированы, выполнено аварийное переключение на исходную топологию.<br>22 октября 16:45 UTC - Повторно включена обработка очереди из более пяти миллионов событий ловушек и 80 тысяч сборок страниц.<br>22 октября 23:03 UTC - Все ожидающие сборки вебхуков и страниц обработаны, и подтверждена целостность и правильная работа всех систем. Статус сайта обновлен до зеленого  |
|  Последующие действия  |  Было проведено: устранение несоответствий данных, улучшение оценки временных прогнозов устранения проблемы.<br>Технические изменения:<br> 1. изменены настройки Orchestrator для предотвращения продвижения первиных баз данных через региональные границы.<br>2. Перешли на новый механизм отчетности о статусах.<br>Ускорили реализация N + 1 резервирование на уровне датацентра.<br><br>Организационные изменения:<br> 1. внедрение практики chaos engineering  |
